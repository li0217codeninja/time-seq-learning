{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMNy8KRmYbcjLxiyw3LhfPZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/li0217codeninja/time-seq-learning/blob/main/timeseriesTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEQUISXvHJFt",
        "outputId": "a35210fd-be45-40f3-a311-6b5a1daf7b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "fIz62e3DHc3e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Timeseries PyTorch Transformer"
      ],
      "metadata": {
        "id": "ES45mtocEzTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, seq_length):\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000  # Replace with actual dataset size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Generate a sequence number\n",
        "        seq_num = torch.tensor(idx)\n",
        "\n",
        "        # Generate a sequence of numbers based on seq_num\n",
        "        seq_data = torch.randn(self.seq_length) * seq_num\n",
        "\n",
        "        return seq_num, seq_data\n",
        "\n",
        "def collate_fn(samples):\n",
        "    seq_nums, seq_data = zip(*samples)\n",
        "    # Pad or truncate sequences to a fixed length (optional)\n",
        "    # ...\n",
        "    return torch.stack(seq_nums), torch.stack(seq_data)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    SequenceDataset(seq_length=32),\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n"
      ],
      "metadata": {
        "id": "nqSG2spHLgVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model = nn.Transformer(nhead=16)\n",
        "src = torch.rand(10, 32, 512)\n",
        "tgt = torch.rand(20, 32, 512)\n",
        "out = transformer_model(src, tgt)"
      ],
      "metadata": {
        "id": "RQ7q2JjaHkWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters())\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    # Forward pass\n",
        "    output = transformer_model(data,target)\n",
        "    # Loss calculation\n",
        "    loss = loss_fn(output, target)\n",
        "    # Backward pass and parameter update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "3XUyed8OKcDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szgxXweSKx_K",
        "outputId": "2d916fb4-6b9b-43a0-a417-84158c233f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7a4ca5ac67a0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Train loop with LSTM\n",
        "\n"
      ],
      "metadata": {
        "id": "9C7wpfIB839I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNmodel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size,  num_features):\n",
        "    super(RNNmodel,self).__init__() #todo check super()\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size,  batch_first=True)\n",
        "    self.dense = nn.Linear(hidden_size, num_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h, _ = self.lstm(x)\n",
        "    output = self.dense(h[:,-1,:])\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "X6wIQKdU895Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyDataset(Dataset):\n",
        "  def __init__(self,  data, sequence_len):\n",
        "    # 100 sequences, 10 time steps, 2 features per variable, 3 variables\n",
        "    self.data = data\n",
        "    self.sequence_length = sequence_len\n",
        "  def __len__(self):\n",
        "    return len(self.data) - self.sequence_length\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.data[idx:idx + self.sequence_length, :], dtype=torch.float32)\n",
        "        y = torch.tensor(self.data[idx + self.sequence_length, :], dtype=torch.float32)\n",
        "        return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "r3ijYD2b-3pL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pM2Vf3Gk_MMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dummy data\n",
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "sequence_length = 10\n",
        "num_features = 3\n",
        "\n",
        "data = np.random.randn(num_samples, num_features)\n",
        "\n",
        "# Create the model, dataset, and data loader\n",
        "input_size = num_features\n",
        "hidden_size = 50\n",
        "output_size = num_features\n",
        "\n",
        "model = RNNmodel(input_size, hidden_size, output_size)\n",
        "dataset = DummyDataset(data, sequence_length)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Example usage:\n",
        "for epoch in range(10):\n",
        "  for batch_x, batch_y in dataloader:\n",
        "        # Training logic goes here\n",
        "        # e.g., forward pass, loss computation, backward pass, and optimization\n",
        "        outputs = model(batch_x)\n",
        "        loss = nn.MSELoss()(outputs, batch_y)\n",
        "\n",
        "        # Your training steps here\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  print(f'Epoch {epoch + 1}, Batch Loss: {loss.item()}')\n",
        "\n",
        "\n",
        "# After training, you can use the trained model for predictions\n",
        "# For example, if you have a test sequence 'test_sequence':\n",
        "# test_sequence = torch.tensor(test_sequence, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "# prediction = model(test_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zsne0uN_M7o",
        "outputId": "f7f8631a-0a17-4e09-d18a-44ca0ac19428"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch Loss: 1.082436203956604\n",
            "Epoch 2, Batch Loss: 0.8982471823692322\n",
            "Epoch 3, Batch Loss: 0.8221070170402527\n",
            "Epoch 4, Batch Loss: 0.8753069043159485\n",
            "Epoch 5, Batch Loss: 1.306394100189209\n",
            "Epoch 6, Batch Loss: 0.7554345726966858\n",
            "Epoch 7, Batch Loss: 0.8944863677024841\n",
            "Epoch 8, Batch Loss: 0.7750611901283264\n",
            "Epoch 9, Batch Loss: 0.9092220664024353\n",
            "Epoch 10, Batch Loss: 0.7927865982055664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SmIQqM4nASM2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}