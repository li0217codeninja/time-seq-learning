{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNuIg6mYM5VsnAlw54+/wRQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/li0217codeninja/time-seq-learning/blob/main/timeseriesTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEQUISXvHJFt",
        "outputId": "07091a7a-ed5c-4286-cd17-8f968041ff94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "fIz62e3DHc3e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Timeseries PyTorch Transformer"
      ],
      "metadata": {
        "id": "ES45mtocEzTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, seq_length):\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000  # Replace with actual dataset size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Generate a sequence number\n",
        "        seq_num = torch.tensor(idx)\n",
        "\n",
        "        # Generate a sequence of numbers based on seq_num\n",
        "        seq_data = torch.randn(self.seq_length) * seq_num\n",
        "\n",
        "        return seq_num, seq_data\n",
        "\n",
        "def collate_fn(samples):\n",
        "    seq_nums, seq_data = zip(*samples)\n",
        "    # Pad or truncate sequences to a fixed length (optional)\n",
        "    # ...\n",
        "    return torch.stack(seq_nums), torch.stack(seq_data)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    SequenceDataset(seq_length=32),\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n"
      ],
      "metadata": {
        "id": "nqSG2spHLgVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model = nn.Transformer(nhead=16)\n",
        "src = torch.rand(10, 32, 512)\n",
        "tgt = torch.rand(20, 32, 512)\n",
        "out = transformer_model(src, tgt)"
      ],
      "metadata": {
        "id": "RQ7q2JjaHkWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters())\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    # Forward pass\n",
        "    output = transformer_model(data,target)\n",
        "    # Loss calculation\n",
        "    loss = loss_fn(output, target)\n",
        "    # Backward pass and parameter update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "3XUyed8OKcDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szgxXweSKx_K",
        "outputId": "2d916fb4-6b9b-43a0-a417-84158c233f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7a4ca5ac67a0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom train loop with LSTM\n",
        "\n",
        "1.   understand the bi-direction argument in lstm\n",
        "2.   how to dignose if any issue with model trainining\n",
        "3.   one step prediction, can we do multi-step forcast? is it just a for loop.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9C7wpfIB839I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNmodel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size,  num_features):\n",
        "    super(RNNmodel,self).__init__() #todo check super()\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size,  batch_first=True)\n",
        "    self.dense = nn.Linear(hidden_size, num_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h, _ = self.lstm(x) # output, (h_n, c_n)\n",
        "    output = self.dense(h[:,-1,:])\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "X6wIQKdU895Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8UmRdfKOqa5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyDataset(Dataset):\n",
        "  def __init__(self,  data, sequence_len):\n",
        "    # 100 sequences, 10 time steps, 2 features per variable, 3 variables\n",
        "    self.data = data\n",
        "    self.sequence_length = sequence_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data) - self.sequence_length\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.data[idx:idx + self.sequence_length, :], dtype=torch.float32)\n",
        "        y = torch.tensor(self.data[idx + self.sequence_length, :], dtype=torch.float32)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "r3ijYD2b-3pL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pM2Vf3Gk_MMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dummy data\n",
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "sequence_length = 10\n",
        "num_features = 3\n",
        "\n",
        "data = np.random.randn(num_samples, num_features)\n",
        "\n",
        "# Create the model, dataset and data loader\n",
        "input_size = num_features\n",
        "hidden_size = 50\n",
        "output_size = num_features\n",
        "\n",
        "model = RNNmodel(input_size, hidden_size, output_size)\n",
        "dataset = DummyDataset(data, sequence_length)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(100):\n",
        "  for batch_x, batch_y in dataloader:\n",
        "        # Training logic goes here\n",
        "        # e.g., forward pass, loss computation, backward pass, and optimization\n",
        "        outputs = model(batch_x)\n",
        "        loss = nn.MSELoss()(outputs, batch_y)\n",
        "\n",
        "        # Your training steps here\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  print(f'Epoch {epoch + 1}, Batch Loss: {loss.item()}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zsne0uN_M7o",
        "outputId": "a8da0697-e553-4ea7-e593-157f0d4d2f6e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch Loss: 0.7638987898826599\n",
            "Epoch 2, Batch Loss: 1.0971240997314453\n",
            "Epoch 3, Batch Loss: 0.8734275698661804\n",
            "Epoch 4, Batch Loss: 1.013189673423767\n",
            "Epoch 5, Batch Loss: 0.7677677273750305\n",
            "Epoch 6, Batch Loss: 1.1158329248428345\n",
            "Epoch 7, Batch Loss: 0.929412305355072\n",
            "Epoch 8, Batch Loss: 1.2866199016571045\n",
            "Epoch 9, Batch Loss: 0.9599927067756653\n",
            "Epoch 10, Batch Loss: 1.1041592359542847\n",
            "Epoch 11, Batch Loss: 0.9852710366249084\n",
            "Epoch 12, Batch Loss: 1.0059996843338013\n",
            "Epoch 13, Batch Loss: 0.9701211452484131\n",
            "Epoch 14, Batch Loss: 0.9712355136871338\n",
            "Epoch 15, Batch Loss: 1.078649878501892\n",
            "Epoch 16, Batch Loss: 0.9533963203430176\n",
            "Epoch 17, Batch Loss: 0.7716512084007263\n",
            "Epoch 18, Batch Loss: 1.06806480884552\n",
            "Epoch 19, Batch Loss: 1.1129637956619263\n",
            "Epoch 20, Batch Loss: 0.9060471057891846\n",
            "Epoch 21, Batch Loss: 0.9639253616333008\n",
            "Epoch 22, Batch Loss: 1.0287281274795532\n",
            "Epoch 23, Batch Loss: 0.8987563252449036\n",
            "Epoch 24, Batch Loss: 0.9339909553527832\n",
            "Epoch 25, Batch Loss: 1.1793571710586548\n",
            "Epoch 26, Batch Loss: 1.1830796003341675\n",
            "Epoch 27, Batch Loss: 1.002711534500122\n",
            "Epoch 28, Batch Loss: 0.977527379989624\n",
            "Epoch 29, Batch Loss: 1.051510214805603\n",
            "Epoch 30, Batch Loss: 0.7943012118339539\n",
            "Epoch 31, Batch Loss: 0.8932831883430481\n",
            "Epoch 32, Batch Loss: 0.9836270213127136\n",
            "Epoch 33, Batch Loss: 1.2161245346069336\n",
            "Epoch 34, Batch Loss: 1.0190953016281128\n",
            "Epoch 35, Batch Loss: 1.0045912265777588\n",
            "Epoch 36, Batch Loss: 0.878925621509552\n",
            "Epoch 37, Batch Loss: 1.1887966394424438\n",
            "Epoch 38, Batch Loss: 1.1798840761184692\n",
            "Epoch 39, Batch Loss: 0.8315027356147766\n",
            "Epoch 40, Batch Loss: 1.085870623588562\n",
            "Epoch 41, Batch Loss: 0.8652653098106384\n",
            "Epoch 42, Batch Loss: 1.124332070350647\n",
            "Epoch 43, Batch Loss: 1.0706573724746704\n",
            "Epoch 44, Batch Loss: 0.9473050236701965\n",
            "Epoch 45, Batch Loss: 0.995795488357544\n",
            "Epoch 46, Batch Loss: 1.0551947355270386\n",
            "Epoch 47, Batch Loss: 1.0558887720108032\n",
            "Epoch 48, Batch Loss: 0.9224521517753601\n",
            "Epoch 49, Batch Loss: 1.0784894227981567\n",
            "Epoch 50, Batch Loss: 0.9895573258399963\n",
            "Epoch 51, Batch Loss: 0.9206183552742004\n",
            "Epoch 52, Batch Loss: 1.3592567443847656\n",
            "Epoch 53, Batch Loss: 1.075629711151123\n",
            "Epoch 54, Batch Loss: 0.9320163726806641\n",
            "Epoch 55, Batch Loss: 1.0818387269973755\n",
            "Epoch 56, Batch Loss: 0.7633445858955383\n",
            "Epoch 57, Batch Loss: 1.140454649925232\n",
            "Epoch 58, Batch Loss: 0.6938326358795166\n",
            "Epoch 59, Batch Loss: 0.845703125\n",
            "Epoch 60, Batch Loss: 0.9739744067192078\n",
            "Epoch 61, Batch Loss: 0.7470324635505676\n",
            "Epoch 62, Batch Loss: 1.059085726737976\n",
            "Epoch 63, Batch Loss: 0.9356632232666016\n",
            "Epoch 64, Batch Loss: 1.1661821603775024\n",
            "Epoch 65, Batch Loss: 0.9755294919013977\n",
            "Epoch 66, Batch Loss: 0.9000868201255798\n",
            "Epoch 67, Batch Loss: 0.9521946907043457\n",
            "Epoch 68, Batch Loss: 0.7701809406280518\n",
            "Epoch 69, Batch Loss: 0.9520280361175537\n",
            "Epoch 70, Batch Loss: 0.9873464107513428\n",
            "Epoch 71, Batch Loss: 0.9516043066978455\n",
            "Epoch 72, Batch Loss: 1.1028757095336914\n",
            "Epoch 73, Batch Loss: 0.9692328572273254\n",
            "Epoch 74, Batch Loss: 0.8612895607948303\n",
            "Epoch 75, Batch Loss: 0.766475260257721\n",
            "Epoch 76, Batch Loss: 1.028747797012329\n",
            "Epoch 77, Batch Loss: 0.7386264801025391\n",
            "Epoch 78, Batch Loss: 1.012100100517273\n",
            "Epoch 79, Batch Loss: 0.7830110192298889\n",
            "Epoch 80, Batch Loss: 1.0245301723480225\n",
            "Epoch 81, Batch Loss: 0.8235352635383606\n",
            "Epoch 82, Batch Loss: 0.944511890411377\n",
            "Epoch 83, Batch Loss: 0.8429899215698242\n",
            "Epoch 84, Batch Loss: 1.03109610080719\n",
            "Epoch 85, Batch Loss: 0.8800868988037109\n",
            "Epoch 86, Batch Loss: 1.0703333616256714\n",
            "Epoch 87, Batch Loss: 0.9019013047218323\n",
            "Epoch 88, Batch Loss: 0.8547269701957703\n",
            "Epoch 89, Batch Loss: 1.0618005990982056\n",
            "Epoch 90, Batch Loss: 0.9319782257080078\n",
            "Epoch 91, Batch Loss: 0.8630767464637756\n",
            "Epoch 92, Batch Loss: 1.159280776977539\n",
            "Epoch 93, Batch Loss: 0.9667720198631287\n",
            "Epoch 94, Batch Loss: 1.1868114471435547\n",
            "Epoch 95, Batch Loss: 0.9201350808143616\n",
            "Epoch 96, Batch Loss: 0.7773849368095398\n",
            "Epoch 97, Batch Loss: 0.6803310513496399\n",
            "Epoch 98, Batch Loss: 0.7797432541847229\n",
            "Epoch 99, Batch Loss: 0.9686000943183899\n",
            "Epoch 100, Batch Loss: 0.9995473027229309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming model is your PyTorch model\n",
        "initial_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Initial number of parameters:\", initial_params)\n",
        "\n",
        "# Check if any parameters have been updated\n",
        "updated_params = sum(p.numel() for p in model.parameters())\n",
        "if updated_params > initial_params:\n",
        "    print(\"The model has been trained.\")\n",
        "else:\n",
        "    print(\"The model has not been trained.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHZ6YUx8myPn",
        "outputId": "cf092e9c-9f2e-4203-b3f0-fb98c3a95f6c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial number of parameters: 11153\n",
            "The model has not been trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "# After training, you can use the trained model for predictions\n",
        "test_sequence = torch.tensor(np.random.randn(11, num_features), dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "test_sequence\n",
        "prediction = model(test_sequence)\n",
        "\n",
        "mse = nn.MSELoss()(prediction, test_sequence[:,-1,:])\n",
        "mse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cra6nA0LkDWn",
        "outputId": "6fa86cf4-57e9-4477-cdd7-d693cd63949d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9112, grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "print()\n",
        "\n",
        "# Print optimizer's state_dict\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQw8ot8Nqcd0",
        "outputId": "cb7b6845-b51f-46e2-a654-ea20771b49a2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "lstm.weight_ih_l0 \t torch.Size([200, 3])\n",
            "lstm.weight_hh_l0 \t torch.Size([200, 50])\n",
            "lstm.bias_ih_l0 \t torch.Size([200])\n",
            "lstm.bias_hh_l0 \t torch.Size([200])\n",
            "dense.weight \t torch.Size([3, 50])\n",
            "dense.bias \t torch.Size([3])\n",
            "\n",
            "Optimizer's state_dict:\n",
            "state \t {0: {'momentum_buffer': tensor([[ 1.9207e-03,  1.3583e-03,  9.2337e-04],\n",
            "        [ 4.1726e-05,  1.1890e-03,  4.0827e-05],\n",
            "        [-7.4290e-04, -1.2806e-04, -2.7775e-05],\n",
            "        [-2.4266e-04, -8.6451e-04, -5.1736e-04],\n",
            "        [ 1.5325e-04, -2.6185e-03, -9.1021e-04],\n",
            "        [-1.6743e-03,  2.5834e-03,  8.1540e-04],\n",
            "        [ 6.8749e-04, -3.5512e-03,  3.7221e-04],\n",
            "        [ 1.8752e-04, -1.8760e-03,  6.6700e-04],\n",
            "        [-9.9014e-04,  9.0225e-05,  1.8158e-04],\n",
            "        [ 4.7678e-04, -1.4619e-03, -6.3338e-04],\n",
            "        [ 3.9095e-04, -1.0312e-03,  2.9193e-04],\n",
            "        [-7.3947e-04,  1.3365e-03,  1.4538e-03],\n",
            "        [ 1.0970e-03, -3.9511e-05,  4.9406e-04],\n",
            "        [ 4.4125e-04, -7.3113e-04,  2.3710e-04],\n",
            "        [ 5.2031e-04,  1.1802e-03,  2.8864e-04],\n",
            "        [ 1.2315e-03, -9.2080e-04, -1.1254e-03],\n",
            "        [-1.9689e-04,  8.8090e-04,  1.1738e-04],\n",
            "        [-7.5668e-04,  5.3289e-04,  3.6942e-04],\n",
            "        [-4.8668e-04,  3.1894e-03, -2.9357e-03],\n",
            "        [ 1.4226e-03, -3.8798e-04, -5.0025e-04],\n",
            "        [-1.2815e-04, -7.2295e-03,  7.1799e-04],\n",
            "        [-3.4386e-04, -1.1226e-04,  9.8417e-04],\n",
            "        [ 3.0720e-04, -1.9040e-03, -6.2995e-04],\n",
            "        [-4.7474e-04,  2.2389e-04,  2.1991e-04],\n",
            "        [ 2.2217e-04,  2.6931e-04, -5.0423e-04],\n",
            "        [-2.6052e-03,  7.1298e-04, -1.0027e-03],\n",
            "        [ 1.1831e-03, -1.3488e-03,  3.1536e-05],\n",
            "        [ 2.9710e-04, -1.5889e-05, -7.6563e-04],\n",
            "        [ 1.6615e-04, -2.1606e-03, -2.0704e-03],\n",
            "        [-1.3570e-04,  9.4692e-04, -3.6753e-04],\n",
            "        [-9.7108e-04,  2.6823e-03,  1.2390e-03],\n",
            "        [ 3.9357e-05, -8.5339e-05, -7.6983e-04],\n",
            "        [-2.4670e-04,  4.4584e-04,  6.4699e-04],\n",
            "        [-3.7607e-04, -1.4278e-03,  1.9812e-03],\n",
            "        [ 2.1853e-03, -4.7299e-03, -2.8157e-03],\n",
            "        [-2.3087e-04,  7.5172e-04, -6.8856e-05],\n",
            "        [-1.8771e-03,  5.8826e-05, -1.4102e-04],\n",
            "        [ 2.2506e-05,  1.8939e-04, -8.9411e-05],\n",
            "        [ 4.7261e-04, -1.2555e-03, -3.5448e-04],\n",
            "        [-8.5407e-05,  3.0146e-04,  4.6463e-06],\n",
            "        [ 5.1309e-04, -7.4905e-04,  6.7659e-04],\n",
            "        [-5.8606e-03,  3.1492e-03,  1.6029e-03],\n",
            "        [ 1.2725e-03, -1.1567e-03, -4.9815e-04],\n",
            "        [-1.4539e-03,  2.5220e-03,  3.5452e-03],\n",
            "        [-4.1267e-04,  1.0418e-03, -4.1372e-04],\n",
            "        [-2.6238e-04, -9.8288e-04,  5.5085e-04],\n",
            "        [-1.2391e-03,  1.0157e-03,  3.3998e-04],\n",
            "        [ 1.4329e-03, -7.3742e-05, -4.9965e-04],\n",
            "        [-8.7173e-05, -4.4546e-04, -2.9516e-04],\n",
            "        [ 7.9192e-04, -3.4801e-04, -5.2159e-05],\n",
            "        [-3.5171e-04,  2.3719e-04,  2.0300e-04],\n",
            "        [ 6.9216e-04,  8.0749e-04, -1.2343e-04],\n",
            "        [-1.2188e-04,  3.9974e-05, -1.5017e-05],\n",
            "        [ 2.1410e-04, -3.4639e-04, -4.0342e-04],\n",
            "        [-1.7601e-03, -6.1594e-04, -4.9722e-05],\n",
            "        [-9.0836e-04, -7.5531e-04, -2.7832e-06],\n",
            "        [-4.7052e-04,  1.9198e-04,  1.0773e-05],\n",
            "        [-5.5532e-04,  1.2642e-03,  3.5696e-06],\n",
            "        [-4.3073e-04, -1.2688e-04, -2.9438e-05],\n",
            "        [ 1.3426e-03, -5.2225e-04, -1.6594e-03],\n",
            "        [ 3.0643e-05,  4.5121e-04,  1.9807e-05],\n",
            "        [ 4.5759e-04,  4.1260e-04, -1.7359e-04],\n",
            "        [ 4.4005e-04, -7.0477e-04,  9.7499e-05],\n",
            "        [-6.7769e-04,  2.8736e-04,  7.0095e-04],\n",
            "        [-5.1236e-04,  7.3335e-04, -2.6692e-05],\n",
            "        [-8.6323e-04,  4.7949e-04,  8.2192e-04],\n",
            "        [ 4.1411e-04, -5.6972e-04,  1.1160e-05],\n",
            "        [-3.8489e-04, -2.6489e-04,  7.8990e-05],\n",
            "        [-3.7102e-04,  3.7274e-04, -9.3220e-05],\n",
            "        [ 6.9079e-04, -1.2268e-04,  7.0974e-04],\n",
            "        [ 8.7932e-04,  3.7335e-04, -1.1446e-04],\n",
            "        [-4.4476e-05,  1.6196e-04,  2.5652e-05],\n",
            "        [-5.0855e-04, -7.5079e-04,  3.4060e-04],\n",
            "        [ 9.6705e-04, -4.6155e-06, -9.7298e-04],\n",
            "        [-5.9239e-05,  1.1764e-05,  1.1980e-04],\n",
            "        [-6.0016e-04,  1.1070e-04,  5.3148e-04],\n",
            "        [ 3.1930e-05, -3.1650e-04,  1.1364e-04],\n",
            "        [ 1.1375e-04,  3.3468e-04, -2.0022e-05],\n",
            "        [ 1.0329e-05, -4.3475e-04,  3.9341e-04],\n",
            "        [ 4.6581e-04,  1.0751e-03, -1.0365e-05],\n",
            "        [-2.4223e-04, -4.1109e-06,  2.0975e-04],\n",
            "        [-5.4214e-05, -8.4585e-05, -8.4237e-05],\n",
            "        [ 1.0477e-03, -7.4405e-05, -3.8086e-05],\n",
            "        [ 5.8797e-04,  1.3394e-04, -1.6281e-04],\n",
            "        [ 2.8562e-04, -2.5942e-04,  8.7212e-04],\n",
            "        [-3.5139e-04, -3.7580e-05,  2.5942e-05],\n",
            "        [ 5.7038e-04, -7.7580e-04,  4.3093e-04],\n",
            "        [ 1.0757e-05,  1.2907e-04,  5.0467e-05],\n",
            "        [ 4.2696e-04, -4.2528e-05, -2.3457e-05],\n",
            "        [-1.3898e-06,  2.3681e-04,  2.3863e-05],\n",
            "        [ 2.6304e-04,  1.3836e-04,  3.3704e-04],\n",
            "        [-1.0250e-03, -6.6921e-04,  3.9586e-04],\n",
            "        [-3.5519e-04,  1.8561e-04, -6.4420e-05],\n",
            "        [ 3.8729e-04,  1.8487e-04, -7.6155e-04],\n",
            "        [ 5.7835e-05,  1.4114e-04, -1.0960e-04],\n",
            "        [ 6.6295e-05,  5.3852e-05, -1.7092e-04],\n",
            "        [-1.5702e-04, -7.0361e-04,  5.5328e-05],\n",
            "        [ 1.3703e-04,  3.8076e-04,  2.8981e-04],\n",
            "        [-6.1776e-05, -4.5652e-04,  1.8583e-04],\n",
            "        [ 2.1243e-04,  1.8113e-04,  2.4253e-04],\n",
            "        [-2.1337e-03,  6.9926e-04,  2.2256e-03],\n",
            "        [-5.2377e-03,  3.2971e-03,  1.4847e-03],\n",
            "        [ 3.6997e-03, -6.1304e-03, -1.0538e-03],\n",
            "        [ 4.5139e-03,  2.8803e-03, -1.0389e-03],\n",
            "        [ 1.2557e-02, -6.2302e-03, -1.2307e-03],\n",
            "        [-7.3847e-03,  2.2617e-04, -4.5222e-04],\n",
            "        [ 1.1104e-02, -4.7523e-03, -1.8015e-03],\n",
            "        [ 6.3483e-04,  1.1761e-02,  3.4566e-03],\n",
            "        [ 1.0577e-02, -6.8902e-03,  5.5174e-03],\n",
            "        [-8.8878e-03, -7.7259e-03, -2.3411e-03],\n",
            "        [ 2.9318e-03, -4.6567e-03, -3.0080e-03],\n",
            "        [-2.6156e-03,  3.9100e-04,  2.5442e-03],\n",
            "        [-7.7848e-03,  2.4529e-03,  2.6443e-04],\n",
            "        [-3.1605e-03, -5.6360e-03,  2.1494e-04],\n",
            "        [ 3.9680e-03,  1.1656e-02, -3.0893e-03],\n",
            "        [ 2.8199e-03,  4.8613e-03,  2.1980e-03],\n",
            "        [ 2.9885e-03, -4.3346e-03, -1.0600e-04],\n",
            "        [ 5.7673e-03,  6.9162e-03,  4.6554e-03],\n",
            "        [-9.4783e-03, -6.5322e-03, -2.4349e-03],\n",
            "        [-7.1435e-03,  4.3572e-03, -4.5031e-03],\n",
            "        [-1.9720e-03, -1.3412e-02, -1.2504e-03],\n",
            "        [ 3.6019e-03, -2.8573e-03,  1.7769e-03],\n",
            "        [ 2.7966e-03, -5.9217e-03, -2.4017e-04],\n",
            "        [-5.7409e-03, -3.8510e-03, -2.0665e-03],\n",
            "        [-1.9715e-03,  7.8747e-04, -1.3701e-03],\n",
            "        [-1.0809e-02,  9.1302e-03, -5.0474e-04],\n",
            "        [-2.6793e-03, -3.0070e-04, -7.8906e-03],\n",
            "        [ 3.7463e-03, -4.6229e-03,  3.8540e-03],\n",
            "        [-7.7998e-05,  2.8850e-03,  2.4389e-05],\n",
            "        [ 8.3670e-03, -3.3569e-03,  4.2585e-03],\n",
            "        [-1.8690e-03,  1.4408e-03,  7.7405e-04],\n",
            "        [-1.4673e-03, -6.5471e-04, -2.4095e-04],\n",
            "        [-7.9565e-03, -3.2418e-04, -7.5677e-04],\n",
            "        [ 7.5183e-03,  5.0494e-03,  3.7125e-03],\n",
            "        [-7.6855e-03, -3.6554e-03, -1.7448e-04],\n",
            "        [-4.7080e-03,  2.2117e-03,  5.0435e-04],\n",
            "        [ 1.3241e-02, -2.3350e-03,  1.5851e-04],\n",
            "        [-1.3307e-03,  2.2684e-03,  2.5421e-03],\n",
            "        [ 9.0813e-04,  8.1696e-04,  1.7426e-03],\n",
            "        [ 9.0265e-06, -2.1807e-03, -7.2961e-04],\n",
            "        [ 6.7604e-04,  2.4127e-03, -3.5909e-04],\n",
            "        [ 7.8293e-03, -3.9762e-03,  1.4176e-03],\n",
            "        [-1.1125e-02,  6.8695e-03, -2.6658e-03],\n",
            "        [ 4.6336e-03,  3.5693e-03,  3.3149e-03],\n",
            "        [-1.2124e-03, -1.2345e-03, -4.4634e-03],\n",
            "        [-2.1216e-03, -7.2626e-04, -1.0837e-03],\n",
            "        [-2.1001e-03,  6.6130e-03,  3.0106e-03],\n",
            "        [ 3.9432e-03,  4.3676e-03,  4.0164e-04],\n",
            "        [ 3.4470e-03,  6.6373e-03,  1.8211e-05],\n",
            "        [ 1.0068e-02, -1.7465e-04,  4.1656e-03],\n",
            "        [ 1.8730e-03,  1.1284e-03,  1.9252e-03],\n",
            "        [ 1.1367e-03,  1.1080e-03,  4.8612e-05],\n",
            "        [-8.3284e-04, -5.1883e-04,  9.6781e-04],\n",
            "        [ 6.6620e-04, -1.7218e-03, -1.2931e-03],\n",
            "        [-7.6621e-04, -1.7764e-03, -1.7598e-03],\n",
            "        [-2.0976e-03,  2.1211e-03,  2.0183e-04],\n",
            "        [ 2.0469e-04, -1.6200e-03, -6.4670e-04],\n",
            "        [ 6.4646e-04, -2.9971e-04,  7.2855e-04],\n",
            "        [-9.5825e-04,  3.5117e-04, -1.3434e-04],\n",
            "        [ 2.8912e-03, -2.5565e-03, -3.1277e-03],\n",
            "        [ 2.9295e-04, -5.1978e-04,  3.0819e-04],\n",
            "        [-1.1642e-03,  1.1289e-03, -3.6845e-05],\n",
            "        [ 8.0453e-04, -8.6140e-04,  1.0769e-03],\n",
            "        [-5.4634e-04, -4.1135e-04,  1.3055e-03],\n",
            "        [ 5.3865e-04,  1.7827e-03, -4.0698e-04],\n",
            "        [ 7.7410e-04, -7.5501e-04,  3.9791e-04],\n",
            "        [ 2.2052e-04, -2.7542e-04,  1.0985e-04],\n",
            "        [-1.8300e-03, -7.2491e-05,  4.6903e-04],\n",
            "        [-4.6161e-04,  3.1763e-03, -1.6704e-03],\n",
            "        [ 2.0009e-03, -2.3983e-03,  1.7426e-03],\n",
            "        [-1.7898e-03, -5.5839e-03, -1.7422e-03],\n",
            "        [-1.6740e-04,  2.2658e-05,  3.8168e-04],\n",
            "        [ 5.1440e-04, -2.6099e-03,  1.0197e-03],\n",
            "        [ 8.8275e-04, -2.2952e-04, -1.5396e-03],\n",
            "        [ 1.8552e-04,  4.3845e-04, -1.4812e-04],\n",
            "        [-2.4246e-03,  1.4203e-03, -1.7996e-03],\n",
            "        [ 1.9855e-03, -1.6011e-03,  6.3952e-04],\n",
            "        [ 3.2069e-04, -2.9369e-04,  4.0956e-05],\n",
            "        [-6.4745e-04, -2.6437e-03, -1.2140e-03],\n",
            "        [ 2.2543e-04,  1.5341e-03, -3.5787e-04],\n",
            "        [-1.1637e-03,  2.0435e-03,  9.3983e-04],\n",
            "        [-1.0843e-04, -5.7075e-05, -5.9981e-04],\n",
            "        [ 9.7690e-04,  6.2370e-04,  5.2052e-04],\n",
            "        [-1.8457e-04, -1.0891e-03,  8.8832e-04],\n",
            "        [ 1.0306e-03, -4.0417e-03, -1.8855e-03],\n",
            "        [-6.3936e-04,  8.1205e-04,  7.4844e-06],\n",
            "        [-9.9678e-04, -1.0291e-03, -3.0683e-04],\n",
            "        [ 1.0379e-04,  2.5007e-04,  7.5389e-05],\n",
            "        [ 7.5554e-04, -2.0033e-03, -6.6999e-04],\n",
            "        [-1.6588e-04,  5.8354e-04, -8.1741e-05],\n",
            "        [-8.9848e-06, -7.5782e-04, -1.3184e-04],\n",
            "        [-4.9301e-03,  2.3781e-03, -9.6532e-04],\n",
            "        [ 7.3896e-04, -3.9356e-04, -4.6947e-04],\n",
            "        [-5.8722e-04,  2.0165e-03,  1.1679e-03],\n",
            "        [-2.8607e-04,  6.1369e-04, -1.0940e-04],\n",
            "        [-5.3339e-04, -8.3733e-04,  9.5214e-05],\n",
            "        [-1.6732e-03,  2.9491e-04,  7.7479e-04],\n",
            "        [ 1.0639e-03,  2.9171e-04, -1.3852e-04],\n",
            "        [-8.9256e-05, -1.4505e-03, -1.8430e-04],\n",
            "        [ 7.2793e-04, -3.0638e-04,  5.5023e-04]])}, 1: {'momentum_buffer': tensor([[-8.4493e-05, -2.7501e-05, -5.5422e-05,  ..., -4.5460e-05,\n",
            "          1.5695e-05, -3.4255e-05],\n",
            "        [-1.7414e-06, -8.4604e-06,  1.3680e-05,  ...,  9.3267e-06,\n",
            "          1.7338e-05, -9.2559e-07],\n",
            "        [-8.0156e-05, -1.0186e-04,  5.6899e-06,  ..., -6.4057e-06,\n",
            "          3.6500e-05, -1.8936e-05],\n",
            "        ...,\n",
            "        [ 4.5389e-05,  2.6232e-05,  2.8859e-05,  ...,  1.6269e-05,\n",
            "         -2.9434e-05,  1.7641e-05],\n",
            "        [-1.5720e-04,  1.8934e-05, -3.0010e-05,  ..., -7.4554e-05,\n",
            "          1.0335e-04, -7.4169e-05],\n",
            "        [ 1.3701e-05, -5.7885e-06,  2.9369e-05,  ...,  4.5969e-06,\n",
            "         -4.4057e-06,  6.5987e-06]])}, 2: {'momentum_buffer': tensor([-7.1652e-04, -7.8393e-05, -6.4428e-04, -3.7823e-04,  4.4079e-04,\n",
            "        -2.8735e-04, -2.1028e-03,  1.8639e-03, -1.1508e-04, -2.6785e-04,\n",
            "        -5.9785e-04,  7.8976e-04,  2.6335e-03,  8.6650e-05,  2.0861e-03,\n",
            "         2.3829e-03, -4.5597e-04, -4.2522e-04, -1.1093e-03,  8.9240e-04,\n",
            "        -1.2517e-03, -1.6815e-04, -1.3175e-03,  9.7309e-05, -6.8846e-05,\n",
            "        -6.0002e-04,  6.7629e-04, -2.2057e-04, -2.7544e-03, -4.1663e-04,\n",
            "         3.6764e-04, -4.9859e-05,  8.8353e-04,  1.3300e-03,  2.1452e-03,\n",
            "        -1.3289e-04, -6.2512e-04, -8.3750e-05,  2.6399e-05,  6.5442e-04,\n",
            "         1.7126e-03, -1.8990e-03,  5.1133e-04,  5.9953e-04, -1.6535e-05,\n",
            "         1.1301e-04, -2.3990e-04,  7.6758e-04, -1.6510e-03,  2.0559e-04,\n",
            "        -5.3642e-04,  1.7490e-04,  2.1854e-04, -5.4990e-04,  1.3685e-04,\n",
            "        -4.8947e-04, -1.7322e-03,  2.5236e-03, -1.5916e-04, -1.2751e-03,\n",
            "        -7.4406e-04,  7.7737e-04,  2.3007e-03,  6.5959e-04,  2.0919e-03,\n",
            "         2.3971e-03, -8.2893e-04,  3.2964e-04, -1.7275e-04,  1.3251e-03,\n",
            "        -6.7940e-04, -2.7456e-04, -1.3163e-03, -5.3159e-04,  1.3295e-05,\n",
            "         3.6595e-04, -1.9874e-04,  1.9730e-04, -2.6454e-03,  4.5707e-04,\n",
            "         4.9258e-04, -1.6972e-04,  9.9380e-04,  1.3999e-03,  1.8604e-03,\n",
            "        -5.5963e-05, -3.2808e-04, -6.8287e-05,  4.2296e-05,  6.0242e-04,\n",
            "         1.9220e-03, -1.2217e-03,  1.1119e-04,  8.8924e-04,  1.4724e-04,\n",
            "         6.2284e-05,  2.8781e-04,  6.2803e-04, -1.7083e-03,  2.8311e-04,\n",
            "        -8.3028e-03, -3.3796e-04, -9.0997e-03,  6.0877e-03, -3.4583e-03,\n",
            "        -1.9776e-02,  2.0907e-02,  4.5440e-02,  6.4590e-03, -2.4879e-02,\n",
            "         1.8245e-02, -2.2794e-02, -1.8286e-02, -1.6442e-02,  3.5887e-02,\n",
            "         2.7854e-02, -1.2655e-02,  4.6629e-03, -2.0525e-02, -7.8061e-03,\n",
            "        -4.4753e-02,  1.0028e-02, -1.2626e-02, -1.8044e-02, -1.8117e-03,\n",
            "         8.0743e-03,  1.2449e-04, -5.1768e-03, -2.1057e-02, -4.9005e-03,\n",
            "        -7.2604e-03, -7.6473e-03, -1.0516e-02,  2.2476e-02, -4.4857e-02,\n",
            "        -5.7845e-03, -3.5993e-03, -5.9180e-03,  1.5427e-03, -6.7607e-03,\n",
            "         2.3260e-02,  1.1911e-02, -3.0424e-03,  4.4513e-02,  6.9872e-03,\n",
            "        -8.6569e-03, -9.7232e-04,  1.6793e-02,  3.1409e-02,  8.0019e-03,\n",
            "        -7.3786e-04, -1.1571e-04, -7.0847e-04, -2.6793e-04,  7.5455e-04,\n",
            "        -4.2567e-04, -2.1074e-03,  2.1675e-03, -9.9164e-05, -2.1566e-04,\n",
            "        -6.5430e-04,  5.4485e-04,  2.7270e-03,  5.1733e-05,  2.2456e-03,\n",
            "         2.7131e-03, -5.4012e-04, -3.2119e-04, -1.2080e-03,  9.9466e-04,\n",
            "        -1.2284e-03, -1.5251e-04, -1.4485e-03,  5.7693e-06, -5.8461e-05,\n",
            "        -5.9334e-04,  7.7626e-04, -1.9751e-04, -2.8157e-03, -4.3277e-04,\n",
            "        -3.0673e-05, -4.5069e-05,  8.7840e-04,  1.0688e-03,  2.0979e-03,\n",
            "        -5.0470e-05, -6.9101e-04, -7.3887e-05,  2.4738e-05,  6.0513e-04,\n",
            "         1.9081e-03, -1.5292e-03,  5.9329e-04,  6.2602e-04,  6.3965e-05,\n",
            "         4.1340e-05, -1.2687e-04,  5.5012e-04, -1.8795e-03,  1.8687e-04])}, 3: {'momentum_buffer': tensor([-7.1652e-04, -7.8393e-05, -6.4428e-04, -3.7823e-04,  4.4079e-04,\n",
            "        -2.8735e-04, -2.1028e-03,  1.8639e-03, -1.1508e-04, -2.6785e-04,\n",
            "        -5.9785e-04,  7.8976e-04,  2.6335e-03,  8.6650e-05,  2.0861e-03,\n",
            "         2.3829e-03, -4.5597e-04, -4.2522e-04, -1.1093e-03,  8.9240e-04,\n",
            "        -1.2517e-03, -1.6815e-04, -1.3175e-03,  9.7309e-05, -6.8846e-05,\n",
            "        -6.0002e-04,  6.7629e-04, -2.2057e-04, -2.7544e-03, -4.1663e-04,\n",
            "         3.6764e-04, -4.9859e-05,  8.8353e-04,  1.3300e-03,  2.1452e-03,\n",
            "        -1.3289e-04, -6.2512e-04, -8.3750e-05,  2.6399e-05,  6.5442e-04,\n",
            "         1.7126e-03, -1.8990e-03,  5.1133e-04,  5.9953e-04, -1.6535e-05,\n",
            "         1.1301e-04, -2.3990e-04,  7.6758e-04, -1.6510e-03,  2.0559e-04,\n",
            "        -5.3642e-04,  1.7490e-04,  2.1854e-04, -5.4990e-04,  1.3685e-04,\n",
            "        -4.8947e-04, -1.7322e-03,  2.5236e-03, -1.5916e-04, -1.2751e-03,\n",
            "        -7.4406e-04,  7.7737e-04,  2.3007e-03,  6.5959e-04,  2.0919e-03,\n",
            "         2.3971e-03, -8.2893e-04,  3.2964e-04, -1.7275e-04,  1.3251e-03,\n",
            "        -6.7940e-04, -2.7456e-04, -1.3163e-03, -5.3159e-04,  1.3295e-05,\n",
            "         3.6595e-04, -1.9874e-04,  1.9730e-04, -2.6454e-03,  4.5707e-04,\n",
            "         4.9258e-04, -1.6972e-04,  9.9380e-04,  1.3999e-03,  1.8604e-03,\n",
            "        -5.5963e-05, -3.2808e-04, -6.8287e-05,  4.2296e-05,  6.0242e-04,\n",
            "         1.9220e-03, -1.2217e-03,  1.1119e-04,  8.8924e-04,  1.4724e-04,\n",
            "         6.2284e-05,  2.8781e-04,  6.2803e-04, -1.7083e-03,  2.8311e-04,\n",
            "        -8.3028e-03, -3.3796e-04, -9.0997e-03,  6.0877e-03, -3.4583e-03,\n",
            "        -1.9776e-02,  2.0907e-02,  4.5440e-02,  6.4590e-03, -2.4879e-02,\n",
            "         1.8245e-02, -2.2794e-02, -1.8286e-02, -1.6442e-02,  3.5887e-02,\n",
            "         2.7854e-02, -1.2655e-02,  4.6629e-03, -2.0525e-02, -7.8061e-03,\n",
            "        -4.4753e-02,  1.0028e-02, -1.2626e-02, -1.8044e-02, -1.8117e-03,\n",
            "         8.0743e-03,  1.2449e-04, -5.1768e-03, -2.1057e-02, -4.9005e-03,\n",
            "        -7.2604e-03, -7.6473e-03, -1.0516e-02,  2.2476e-02, -4.4857e-02,\n",
            "        -5.7845e-03, -3.5993e-03, -5.9180e-03,  1.5427e-03, -6.7607e-03,\n",
            "         2.3260e-02,  1.1911e-02, -3.0424e-03,  4.4513e-02,  6.9872e-03,\n",
            "        -8.6569e-03, -9.7232e-04,  1.6793e-02,  3.1409e-02,  8.0019e-03,\n",
            "        -7.3786e-04, -1.1571e-04, -7.0847e-04, -2.6793e-04,  7.5455e-04,\n",
            "        -4.2567e-04, -2.1074e-03,  2.1675e-03, -9.9164e-05, -2.1566e-04,\n",
            "        -6.5430e-04,  5.4485e-04,  2.7270e-03,  5.1733e-05,  2.2456e-03,\n",
            "         2.7131e-03, -5.4012e-04, -3.2119e-04, -1.2080e-03,  9.9466e-04,\n",
            "        -1.2284e-03, -1.5251e-04, -1.4485e-03,  5.7693e-06, -5.8461e-05,\n",
            "        -5.9334e-04,  7.7626e-04, -1.9751e-04, -2.8157e-03, -4.3277e-04,\n",
            "        -3.0673e-05, -4.5069e-05,  8.7840e-04,  1.0688e-03,  2.0979e-03,\n",
            "        -5.0470e-05, -6.9101e-04, -7.3887e-05,  2.4738e-05,  6.0513e-04,\n",
            "         1.9081e-03, -1.5292e-03,  5.9329e-04,  6.2602e-04,  6.3965e-05,\n",
            "         4.1340e-05, -1.2687e-04,  5.5012e-04, -1.8795e-03,  1.8687e-04])}, 4: {'momentum_buffer': tensor([[ 2.9449e-02, -3.5602e-03,  1.3888e-02, -2.0877e-02, -2.1129e-02,\n",
            "          6.7343e-03, -2.6025e-02,  1.5817e-02, -7.1978e-03,  1.9645e-02,\n",
            "         -9.5937e-03, -1.1767e-02, -3.7037e-02, -8.8973e-03,  2.1035e-02,\n",
            "          3.3801e-02,  1.5737e-02,  6.2618e-03,  1.0057e-02, -4.8663e-02,\n",
            "          1.4194e-02, -6.5392e-03,  2.8417e-02,  1.3606e-02,  1.1878e-02,\n",
            "          7.7135e-03, -1.5436e-02,  6.9846e-03,  3.2133e-02,  2.8577e-03,\n",
            "         -1.5895e-02,  4.9042e-03, -2.7753e-02,  2.3424e-02, -1.7278e-02,\n",
            "          4.9422e-03,  1.0814e-02,  5.0991e-03,  2.1124e-03, -2.6806e-02,\n",
            "          2.8149e-02, -3.4889e-02,  7.4038e-03,  6.5025e-03,  3.7590e-04,\n",
            "         -3.9910e-04, -8.2544e-03,  7.8332e-03, -1.5851e-02,  9.6724e-03],\n",
            "        [ 3.2572e-02,  1.0140e-02,  1.5459e-02, -1.9697e-02, -1.0538e-02,\n",
            "          1.7406e-03, -2.9476e-02,  7.6347e-03, -7.0275e-03,  1.1943e-02,\n",
            "         -1.4239e-02, -2.2217e-02, -4.3846e-02, -8.5492e-04,  1.9132e-02,\n",
            "          1.7305e-02,  6.5826e-03, -3.6473e-03,  9.1438e-03, -4.6364e-02,\n",
            "          3.8557e-03, -8.7183e-03,  3.4159e-02,  2.1724e-03,  9.8338e-03,\n",
            "          9.6216e-03, -2.6194e-02,  9.2182e-03,  3.1299e-02, -3.8732e-03,\n",
            "         -2.2787e-02,  1.5293e-03, -2.4066e-02,  2.2638e-02, -1.7865e-02,\n",
            "         -8.2681e-04,  5.5981e-03,  9.6397e-03,  6.0618e-03, -2.2630e-02,\n",
            "          3.0218e-02, -2.8750e-02, -1.8412e-03,  5.6406e-03,  1.1883e-03,\n",
            "         -1.1049e-02, -1.4084e-02,  1.1082e-02, -1.0429e-02,  1.0176e-02],\n",
            "        [ 1.2672e-03, -5.8979e-03,  9.7584e-03, -1.0734e-02, -3.3980e-03,\n",
            "          2.0502e-03, -1.1883e-02,  1.0868e-02, -3.5440e-03,  1.0209e-03,\n",
            "         -4.1365e-03, -5.1540e-03, -1.5168e-02, -3.0633e-03,  1.1092e-02,\n",
            "          1.4666e-02,  7.0511e-03,  2.7628e-03,  5.9766e-03, -1.2675e-02,\n",
            "          1.0489e-02, -2.4726e-03,  1.7032e-02, -1.0799e-03,  4.6685e-03,\n",
            "         -7.0235e-03, -1.7226e-03,  5.4833e-03,  2.2944e-02,  4.2904e-03,\n",
            "         -1.1802e-02,  3.7964e-03, -2.3185e-02,  6.9765e-03,  4.0891e-03,\n",
            "          1.2005e-03,  1.2102e-02,  7.5671e-05,  8.7632e-05, -1.1245e-02,\n",
            "          9.6914e-03, -1.6781e-03,  9.4225e-03,  6.9190e-03,  6.7499e-04,\n",
            "          1.1576e-02, -1.6123e-03,  2.3354e-03, -1.4465e-02,  5.1930e-03]])}, 5: {'momentum_buffer': tensor([0.3386, 0.3126, 0.1491])}}\n",
            "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5]}]\n"
          ]
        }
      ]
    }
  ]
}